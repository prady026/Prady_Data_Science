sms_raw$type = factor(sms_raw$type)
str(sms_raw)
prop.table(table(sms_raw))
prop.table(table(sms_raw$type))
str(sms_raw)
sms_raw=read.csv(file="F:/Data Science/DS 14/Unit 3/NaiveBayes/SMS-SPAM-BAYES.csv",header=T,sep=",",stringsAsFactors = F)
names(sms_raw)
str(sms_raw)
sms_raw$type = factor(sms_raw$type)
str(sms_raw)
prop.table(table(sms_raw$type))
sms_raw[1:5,]
sms_raw[1:5,]
sms_corpus=VCorpus(VectorSource(sms_raw))
library(tm)
sms_corpus=VCorpus(VectorSource(sms_raw))
inspect(sms_corpus[1:5])
sms_corpus=VCorpus(VectorSource(sms_raw$text))
inspect(sms_corpus[1:5])
head(sms_corpus)
corpus_clean= tm_map(sms_corpus,tolower)
corpus_clean= tm_map(sms_corpus,PlainTextDocument)
corpus_clean= tm+map(sms_corpus,removeNumbers)
corpus_clean= tm_map(sms_corpus,removeNumbers)
corpus_clean= tm_map(sms_corpus,removeWords,stopwords())
corpus_clean= tm_map(sms_corpus,removePunctuation)
corpus_clean= tm_map(sms_corpus,stripWhitespace)
sms_dtm=TermDocumentMatrix(corpus_clean)
dim(sms_dtm)
inspectI(sms_dtm[5,5])
inspect(sms_dtm[5,5])
inspect(sms_dtm[1:5,1:5])
inspect(sms_dtm[1:10,1:10])
dim(sms_dtm)
str(sms_dtm)
inspect(sms_dtm[1:20,1:10])
inspect(sms_dtm[1:20,1:20])
inspect(sms_dtm[1:50,1:20])
inspect(sms_dtm[1:50,1:50])
inspect(sms_dtm[1:5,1:5])
inspect(sms_dtm[1:5,1:10])
inspect(sms_dtm[1:20,1:20])
inspect(sms_dtm[1:10,1:10])
inspect(sms_dtm[1:20,5:20])
inspect(sms_dtm[1:20,1:20])
wordcloud(corpus_clean,min.freq=40,random.order=F,colors=brewer.pal(8, "Dark2"))
install.packages("wordcloud")
library(RColorBrewer)
wordcloud(corpus_clean,min.freq=40,random.order=F,colors=brewer.pal(8, "Dark2"))
library(wordcloud)
str(sms_dtm)wordcloud(corpus_clean,min.freq=40,random.order=F,colors=brewer.pal(8, "Dark2"))
wordcloud(corpus_clean,min.freq=40,random.order=F,colors=brewer.pal(8, "Dark2"))
wordcloud(corpus_clean,min.freq=10,random.order=F,colors=brewer.pal(8, "Dark2"))
wordcloud(corpus_clean,min.freq=10,random.order=F,colors=brewer.pal(8, "Dark2"))
wordcloud(corpus_clean,min.freq=100,random.order=F,colors=brewer.pal(8, "Dark2"))
wordcloud(corpus_clean,min.freq=200,random.order=F,colors=brewer.pal(8, "Dark2"))
wordcloud(corpus_clean,min.freq=500,random.order=F,colors=brewer.pal(8, "Dark2"))
wordcloud(corpus_clean,min.freq=500,random.order=F,colors=brewer.pal(8, "Dark2"))
library(wordcloud)
wordcloud(corpus_clean,min.freq=500,random.order=F,colors=brewer.pal(8, "Dark2"))
sms_raw=read.csv(file="F:/Data Science/DS 14/Unit 3/NaiveBayes/SMS-SPAM-BAYES.csv",header=T,sep=",",stringsAsFactors = F)
prop.table(table(sms_raw$type))
sms_corpus=VCorpus(VectorSource(sms_raw$text))
library(tm)
sms_corpus=VCorpus(VectorSource(sms_raw$text))
corpus_clean= tm_map(sms_corpus,tolower)
corpus_clean= tm_map(sms_corpus,PlainTextDocument)
corpus_clean= tm_map(sms_corpus,removeNumbers)
corpus_clean= tm_map(sms_corpus,removeWords,stopwords())
corpus_clean= tm_map(sms_corpus,removePunctuation)
corpus_clean= tm_map(sms_corpus,stripWhitespace)
sms_dtm=TermDocumentMatrix(corpus_clean)
sms_dtm
dim(sms_dtm)
set.seed(100)
raw_data=sample(2,nrow(sms_raw),replace = TRUE,prob = c(0.7,0.3))
sms_raw_train = sms_raw[raw_data==1,]
sms_raw_test = sms_raw[raw_data==2,]
dim(sms_raw_train)
dim(sms_raw_test)
rm sms_raw_train
rm(sms_raw_train)
rm(sms_raw_test)
sms_raw_test
#Splitting raw data into training and testing data
sms_raw_train=sms_raw[1:4169,]
sms_raw_test=sms_raw[4170:5574,]
#*********************
#Splitting sparse data into training and testing data
sms_dtm_train=sms_dtm[1:4169,]
sms_dtm_test=sms_dtm[4170:5574,]
#*********************
#Splitting corpus data into training and testing data
sms_corpus_train=corpus_clean[1:4169]
sms_corpus_test=corpus_clean[4170:5574]
dim(sms_raw_train)
convert_count=function(x){
x=ifelse(x>0,1,0)
x=factor(x,levels=c(0,1),labels=c("No","Yes"))
return(x)
}
sms_train=apply(sms_train,MARGIN=2,convert_count)
sms_test=apply(sms_test,MARGIN=2,convert_count)
sms_classifier=naiveBayes(sms_train,sms_raw_train$type,laplace=1) # using laplace will replace as 1 with 0 probabilities
#10) Prediction using test data
sms_test_pred=predict(sms_classifier,newdata=sms_test)
convert_count=function(x){
x=ifelse(x>0,1,0)
x=factor(x,levels=c(0,1),labels=c("No","Yes"))
return(x)
}
sms_train=apply(sms_train,MARGIN=2,convert_count)
sms_test=apply(sms_test,MARGIN=2,convert_count)
sms_train=DocumentTermMatrix(sms_corpus_train,list(dictionary = sms_dict))
sms_test=DocumentTermMatrix(sms_corpus_test,list(dictionary = sms_dict))
sms_dict=(findFreqTerms(sms_dtm_train,5))
sms_dict
sms_train=DocumentTermMatrix(sms_corpus_train,list(dictionary = sms_dict))
sms_test=DocumentTermMatrix(sms_corpus_test,list(dictionary = sms_dict))
convert_count=function(x){
x=ifelse(x>0,1,0)
x=factor(x,levels=c(0,1),labels=c("No","Yes"))
return(x)
}
sms_train=apply(sms_train,MARGIN=2,convert_count)
sms_test=apply(sms_test,MARGIN=2,convert_count)
sms_classifier=naiveBayes(sms_train,sms_raw_train$type,laplace=1)
library(e1071)#naive Bayes function
library(gmodels)
library(RColorBrewer)
sms_classifier=naiveBayes(sms_train,sms_raw_train$type,laplace=1)
sms_test_pred=predict(sms_classifier,newdata=sms_test)
sms_test_pred
Performance=CrossTable(sms_raw_test$type,sms_test_pred,prop.r=T,prop.t=F,prop.c=F,dnn=c("Actual","Predicted"))
Accuracy=mean(sms_raw_test$type==sms_test_pred)
Accuracy
Performance=CrossTable(sms_raw_test$type,sms_test_pred,prop.r=T,prop.t=F,prop.c=F,dnn=c("Actual","Predicted"))
install.packages("class")
install.packages("gmodels")
library(class)
library(gmodels)
wbcd=read.table(file="F:/Data Science/DS 14/Unit 3/KNN/wisc_bc_data-KNN.csv",header=T,sep=",")
dim(wbcd)
str(wbcd)
table(wbcd$diagnosis)
wbcd=wbcd[,-1]
str(wbcd)
wbcd$diagnosis=(wbcd$diagnosis,levels=c("B","M"),labels=c("Benign","Malignant"))
wbcd$diagnosis=factor(wbcd$diagnosis,levels=c("B","M"),labels=c("Benign","Malignant"))
str(wbcd)
wbcd$diagnosis
dim(wbcd)
table(wbcd$diagnosis)
normalize = function(x){
return((x-min(x))/(max(x)-min(x)))
}
wbcd= as.data.frame(apply[,2:31],margin=2,normalize)
normalize = function(x){
return((x-min(x))/(max(x)-min(x)))
}
wbcd= as.data.frame(apply(wbcd[,2:31],margin=2,normalize))
normalize = function(x){
return((x-min(x))/(max(x)-min(x)))
}
wbcd= as.data.frame(apply(wbcd[,2:31],MARGIN =2,normalize))
wbcd_train = wbcd[1:469,]
wbcd_test = wbcd[470:569,]
head(wbcd_train)
head(wbcd_train$diagnosis)
normalize = function(x){
return((x-min(x))/(max(x)-min(x)))
}
wbcd_n= as.data.frame(apply(wbcd[,2:31],MARGIN =2,normalize))
rm(wbcd)
rm(wbcd_test)
rm(wbcd_train)
wbcd=read.table(file="F:/Data Science/DS 14/Unit 3/KNN/wisc_bc_data-KNN.csv",header=T,sep=",")
wbcd=wbcd[,-1]
wbcd$diagnosis=factor(wbcd$diagnosis,levels=c("B","M"),labels=c("Benign","Malignant"))
normalize = function(x){
return((x-min(x))/(max(x)-min(x)))
}
wbcd_n= as.data.frame(apply(wbcd[,2:31],MARGIN =2,normalize))
wbcd_train = wbcd_n[1:469,]
wbcd_test = wbcd_n[470:569,]
head(wbcd_train$diagnosis)
wbcd_train_labels=wbcd[1:469,1]
wbcd_test_labels=wbcd[470:569,1]
wbcd_train_labels
wbcd_train
help(knn)
wbcd_pred=knn(train=wbcd_train,test=wbcd_test,cl=wbcd_train_labels,k=21)
wbcd_pred
help("CrossTable")
Performance=CrossTable(wbcd_test_labels,wbcd_pred,prop.t=F,prop.c=F,prop.r=T,dnn=c("Actual","Predict"))
Accuracy=(Performance$t[1,1]+Performance$t[2,2])/length(wbcd_test_labels)
Accuracy
mean(wbcd_pred == wbcd_test_labels)
library(caret)
confusionMatrix(wbcd_pred, wbcd_test_labels)
x=confusionMatrix(wbcd_pred, wbcd_test_labels)
install.packages("class")
install.packages("gmodels")
library(class)
library(gmodels)
wbcd=read.table(file="F:/Data Science/DS 14/Unit 3/KNN/wisc_bc_data-KNN.csv",header=T,sep=",")
dim(wbcd)
str(wbcd)
table(wbcd$diagnosis)
wbcd=wbcd[,-1]
wbcd$diagnosis=factor(wbcd$diagnosis,levels=c("B","M"),labels=c("Benign","Malignant"))
normalize = function(x){
return((x-min(x))/(max(x)-min(x)))
}
wbcd_n= as.data.frame(apply(wbcd[,2:31],MARGIN =2,normalize))
wbcd_train = wbcd_n[1:469,]
wbcd_test = wbcd_n[470:569,]
wbcd_train_labels=wbcd[1:469,1]
wbcd_test_labels=wbcd[470:569,1]
wbcd_pred=knn(train=wbcd_train,test=wbcd_test,cl=wbcd_train_labels,k=21)
Performance=CrossTable(wbcd_test_labels,wbcd_pred,prop.t=F,prop.c=F,prop.r=T,dnn=c("Actual","Predict"))
Accuracy=(Performance$t[1,1]+Performance$t[2,2])/length(wbcd_test_labels)
Accuracy
mean(wbcd_pred == wbcd_test_labels
mean(wbcd_pred == wbcd_test_labels)
mean(wbcd_pred == wbcd_test_labels)
library(caret)
confu = confusionMatrix(wbcd_pred, wbcd_test_labels)
test_pred <- predict(knn_fit, newdata = test)
Auto=read.delim(file="F:/Data Science/DS 14/Unit 2/Resampling/Auto.csv",header=T,sep=",")
dim(Auto)
names(Auto)
summary(Auto)
str(Auto)
plot(Auto$mpg,Auto$horsepower)
cv.error <- c()
for(i in 1:6) {
set.seed (1)
train=sample (392 ,196)
lm.fit=lm(mpg~poly(horsepower ,i),data=Auto,subset = train)
cv.error[i]=mean((mpg-predict(lm.fit,Auto))[-train ]^2)
}
cv.error
install.packages("ISLR")
library (ISLR)
cv.error <- c()
for(i in 1:6) {
set.seed (1)
train=sample (392 ,196)
lm.fit=lm(mpg~poly(horsepower ,i),data=Auto,subset = train)
cv.error[i]=mean((mpg-predict(lm.fit,Auto))[-train ]^2)
}
cv.error
attach(Auto)
cv.error <- c()
for(i in 1:6) {
set.seed (1)
train=sample (392 ,196)
lm.fit=lm(mpg~poly(horsepower ,i),data=Auto,subset = train)
cv.error[i]=mean((mpg-predict(lm.fit,Auto))[-train ]^2)
}
cv.error
plot(x = 1:6, y = cv.error, type='b',xlab = "Polynomial Degree", ylab = "Mean squarred Error")
plot(mpg,horsepower)
dim(train)
set.seed (1) #To Keep dataset consistency
train=sample (392 ,196)
dim(train)
head(train)
summary(train)
plot(x = 1:6, y = cv.error, type='b',xlab = "Polynomial Degree", ylab = "Mean squarred Error")
plot(x = 1:6, y = cv.error,xlab = "Polynomial Degree", ylab = "Mean squarred Error")
plot(x = 1:6, y = cv.error, type='b',xlab = "Polynomial Degree", ylab = "Mean squarred Error")
library (ISLR)
install.packages("boot")
library (boot)
Auto=read.delim(file="F:/Data Science/DS 14/Unit 2/Resampling/Auto.csv",header=T,sep=",")
dim(Auto)
names(Auto)
summary(Auto)
str(Auto)
attach(Auto)
cv.error=c()
for (i in 1:5)
{
glm.fit=glm(mpg~poly(horsepower ,i),data=Auto)
cv.error[i]=cv.glm (Auto ,glm.fit)$delta [1]
}
cv.error
plot(x = 1:5, y = cv.error, type='b',xlab = "Polynomial Degree", ylab = "Cross Validation Error", main = "LOOCV-Bias / Variance Tradeoff")
Auto=read.delim(file="F:/Data Science/DS 14/Unit 2/Resampling/Auto.csv",header=T,sep=",")
dim(Auto)
names(Auto)
summary(Auto)
str(Auto)
attach(Auto)
set.seed(1)
cv.error=c()
for (i in 1:5) {
glm.fit = glm(mpg ~ poly(horsepower,i), data=Auto)
cv.error[i] = round(cv.glm(Auto, glm.fit,K=10)$delta[1],2)
}
cv.error
plot(x = 1:5, y = cv.error, type='b',xlab = "Polynomial Degree", ylab = "Cross Validation Error", main = "K-Fold Bias/Variance Tradeoff")
set.seed (2)
x=matrix (rnorm (50*2) , ncol =2)
dim(x)
head(x)
plot(x[,1],x[,2],pch=18,col="red")
x[1:25 ,1]=x[1:25 ,1]+3
x[1:25 ,2]=x[1:25 ,2]-4
plot(x[,1],x[,2],pch=18,col="red")
hc.single   =hclust(dist(x), method ="single")
plot(hc.single , main=" Single Linkage ", xlab="", sub ="",cex =.9,col="darkgreen")
cutree (hc.single , 2)
z = cutree (hc.single , 2)
z
hc.single   =hclust(dist(x), method ="single")
plot(hc.single , main=" Single Linkage ", xlab="", sub ="",cex =.9,col="darkgreen")
cutree (hc.single , 5)
hc.single   =hclust(dist(x), method ="single")
plot(hc.single , main=" Single Linkage ", xlab="", sub ="",cex =.9,col="darkgreen")
cutree (hc.single , 5)
hc.single   =hclust(dist(x), method ="single")
plot(hc.single , main=" Single Linkage ", xlab="", sub ="",cex =.9,col="darkgreen")
cutree (hc.single , 2)
hc.complete =hclust(dist(x), method ="complete")
plot(hc.complete ,main =" Complete Linkage ", xlab="", sub ="",cex =0.9,col="blue")
cutree (hc.complete , 2)
hc.complete =hclust(dist(x), method ="complete")
plot(hc.complete ,main =" Complete Linkage ", xlab="", sub ="",cex =0.9,col="blue")
cutree (hc.complete , 2)
hc.complete =hclust(dist(x), method ="average")
plot(hc.complete ,main =" average Linkage ", xlab="", sub ="",cex =0.9,col="orange")
cutree (hc.complete , 2)
hc.complete <- hclust(dist(x), method = "complete")
plot(hc.complete, main ="complete linkage", cex = 1, col = "orange")
cutree(hc.complete,2)
set.seed (2)
x=matrix (rnorm (50*2) , ncol =2)
dim(x)
head(x)
plot(x[,1],x[,2],pch=18,col="red")
x[1:25 ,1]=x[1:25 ,1]+3
x[1:25 ,2]=x[1:25 ,2]-4
plot(x[,1],x[,2],pch=18,col="red")
hc.complete <- hclust(dist(x), method = "complete")
plot(hc.complete, main ="complete linkage", cex = 1, col = "orange")
cutree(hc.complete,2)
hc.single   =hclust(dist(x), method ="single")
plot(hc.single , main=" Single Linkage ", xlab="", sub ="",cex =.9,col="darkgreen")
cutree (hc.single , 2)
hc.complete <- hclust(dist(x), method = "complete")
plot(hc.complete, main ="complete linkage", cex = 1, col = "orange")
cutree(hc.complete,2)
set.seed (2)
x=matrix (rnorm (50*2) , ncol =2)
dim(x)
plot(x[,1],x[,2],pch=18,col="red")
x[1:25 ,1]=x[1:25 ,1]+3
x[1:25 ,2]=x[1:25 ,2]-4
plot(x[,1],x[,2],pch=18,col="red")
km.out = kmeans(x,2)
km.out$cluster
plot(x, col=(km.out$cluster), main = "K-means clustering with k = 2",pch = 20, cex = 2)
set.seed (4)
km.out =kmeans (x,3, nstart =1)
km.out$tot.withinss
plot(x, col =(km.out$cluster) , main="K-Means Clustering Results with K", xlab ="", ylab="", pch =20, cex =2)
set.seed (4)
km.out =kmeans (x,3, nstart =1)
km.out$cluster
km.out$tot.withinss
plot(x, col =(km.out$cluster) , main="K-Means Clustering Results with K", xlab ="", ylab="", pch =20, cex =2)
set.seed (4)
km.out =kmeans (x,3, nstart =20)
km.out$cluster
km.out$tot.withinss
plot(x, col =(km.out$cluster) , main="K-Means Clustering Results with K", xlab ="", ylab="", pch =20, cex =2)
set.seed (4)
km.out =kmeans (x,3, nstart =50)
km.out$cluster
km.out$tot.withinss
set.seed (4)
km.out =kmeans (x,3, nstart =29)
km.out$cluster
km.out$tot.withinss
install.packages("tree")
install.packages("ISLR")
library(tree)
library(ISLR)
attach(Carseats)
dim(Carseats)
names(carseats)
head(carseats)
rtree = attach(Carseats)
rtree = read.csv(Carseats)
rtree = read.csv("Carseats")
rtree = read.table("Carseats")
head(Carseats)
names(Carseats)
High = ifelse(Sales <=8, "No", "Yes")
Carseats = data.frame(High)
names(Carseats)
attach(Carseats)
library(ISLR)
attach(Carseats)
dim(Carseats)
rm(High)
dim(Carseats)
install.packages("tree")
install.packages("ISLR")
library(tree)
library(ISLR)
install.packages("tree")
attach(Carseats)
dim(Carseats)
names(Carseats)
head(Carseats)
Carseats = data.frame(Carseats,High)
head(Carseats)
library("ISLR", lib.loc="~/R/win-library/3.6")
detach("package:ISLR", unload=TRUE)
install.packages("tree")
install.packages("ISLR")
library(tree)
library(ISLR)
attach(Carseats)
dim(Carseats)
names(Carseats)
head(Carseats)
High = ifelse(Sales <=8, "No", "Yes")
Carseats = data.frame(Carseats,High)
head(Carseats)
class.tree = tree(High~.-Sales,data = Carseats)
names(class.tree)
summary(class.tree)
plot(class.tree)
text(class.tree,pretty=0)
class.tree
set.seed(2)
train=sample(1:nrow(Carseats),200)
Carseats.test=Carseats[-train,]
High.test=High[-train]
tree.carseats=tree(High~.-Sales,Carseats,subset=train)
tree.pred=predict(tree.carseats,Carseats.test,type="class")
table(tree.pred,High.test
)
mean(tree.pred==High.test)
set.seed(3)
tree.carseats=tree(High~.-Sales,Carseats,subset=train)
tree.cv=cv.tree(tree.carseats,FUN=prune.misclass) # FUN default is deviance
names(tree.cv)
tree.cv
plot(tree.cv$size,tree.cv$dev,type="b")
Prune.tree=prune.misclass(tree.carseats,best=9)
plot(Prune.tree)
text(Prune.tree,pretty=0)
Prunetree.pred=predict(Prune.tree,Carseats.test,type='class')
table(Prunetree.pred,High.test)
mean(Prunetree.pred==High.test)
setwd("F:/Data Science/DS 14/Unit 2/SLR")
dataset = read.csv('Advertising.csv')
View(dataset)
dataset = read.csv('Viralcount_Drug.csv')
View(dataset)
lin_reg = lm(drug ~ viral.count, data = dataset)
View(lin_reg)
summary(lin_reg)
library(ggplot2)
geom_point(aes(x = dataset$drug, y = dataset$viral.count), colour = 'red')
ggplot()+
geom_point(aes(x = dataset$drug, y = dataset$viral.count), colour = 'red')+
geom.line(aes(x = dataset$drug, y = predict(lin_reg, newdata = dataset)), colour = 'blue')+
ggtitle('Drug vs Viral_Count')+
xlab('Drug')+
ylab('Viral_Count')
ggplot()+
geom_point(aes(x = dataset$drug, y = dataset$viral.count), colour = 'red')+
geom_line(aes(x = dataset$drug, y = predict(lin_reg, newdata = dataset)), colour = 'blue')+
ggtitle('Drug vs Viral_Count')+
xlab('Drug')+
ylab('Viral_Count')
lin_reg = lm( viral.count~drug, data = dataset)
summary(lin_reg)
ggplot()+
geom_point(aes(x = dataset$viral.count, y = dataset$drug), colour = 'red')+
geom_line(aes(x = dataset$viral.count, y = predict(lin_reg, newdata = dataset)), colour = 'blue')+
ggtitle('viral.count vs drug')+
xlab('viral.count')+
ylab('drug')
ggplot()+
geom_point(aes(x = dataset$drug, y = dataset$viral.count), colour = 'red')+
geom_line(aes(x = dataset$drug, y = predict(lin_reg, newdata = dataset)), colour = 'blue')+
ggtitle('drug vs viral.count')+
xlab('drug')+
ylab('viral.count')
y_pred = predict(lin_reg, data.frame(viral.count = 2000))
y_pred = predict(lin_reg, data.frame(dataset$viral.count = 2000))
y_pred = predict(lin_reg, data.frame(drug=10))
setwd("F:/Data Science/DS 14/Unit 2")
dataset = read.csv('Advertising.csv')
dataset = read.csv('Advertising.csv')
setwd("F:/Data Science/DS 14/Unit 2/SLR")
dataset = read.csv('Advertising.csv')
View(dataset)
summary(dataset)
regressor = lm(formula = sales ~ TV + radio + newspaper, data = dataset)
View(regressor)
summary(regressor)
regressor1 = lm(formula = sales ~ TV + radio, data = dataset)
summary(regressor1)
